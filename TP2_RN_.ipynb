{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Redes Neuronales 2021\n",
    "\n",
    "Integrantes de grupo:\n",
    "\n",
    "Müller, Malena\n",
    "\n",
    "Scala, Tobías \n",
    "# TP2: Sesgos en el dataset de SNLI\n",
    "\n",
    "El trabajo practico consiste en predecir las ventas diarias para los proximos 6 meses de Rossmann, que opera en 3000 locales en 7 paises europeos. Las ventas son influenciadas por varios factores; incluyendo promociones, competencia, el periodo escolar, fechas festivas, temporada y ubicacion. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importacion de datos\n",
    "\n",
    "En esta primera etapa, levantamos los datos provenientes de los archivos de train, test y store, que corresponden al periodo de tiempo a los años entre el 2013 y el 2015. Luego chequeamos si faltan datos en ciertos campos de cada linea que fue levantada de los archivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tobias Scala\\anaconda3\\envs\\rn\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3441: DtypeWarning: Columns (7) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train[Customers]: NA =  0\n",
      "train[Open]: NA =  0\n",
      "train[Promo]: NA =  0\n",
      "train[StateHoliday]: NA =  0\n",
      "train[SchoolHoliday]: NA =  0\n",
      "test[Open]: NA =  11\n",
      "test[Promo]: NA =  0\n",
      "test[StateHoliday]: NA =  0\n",
      "test[SchoolHoliday]: NA =  0\n",
      "store[StoreType]: NA =  0\n",
      "store[Assortment]: NA =  0\n",
      "store[CompetitionDistance]: NA =  3\n",
      "store[Promo2]: NA =  0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import datetime\n",
    "#from isoweek import Week\n",
    "\n",
    "trainFile = pd.read_csv('train.csv')\n",
    "#print(trainFile.columns)\n",
    "testFile = pd.read_csv('test.csv')\n",
    "#print(trainFile.columns)\n",
    "storeFile = pd.read_csv('store.csv')\n",
    "#print(trainFile.columns)\n",
    "\n",
    "#Me fijo si faltan datos en TRAIN.\n",
    "print('train[Customers]: NA = ', sum(trainFile['Customers'].isna()))\n",
    "print('train[Open]: NA = ', sum(trainFile['Open'].isna()))\n",
    "print('train[Promo]: NA = ', sum(trainFile['Promo'].isna()))\n",
    "print('train[StateHoliday]: NA = ', sum(trainFile['StateHoliday'].isna()))\n",
    "print('train[SchoolHoliday]: NA = ', sum(trainFile['SchoolHoliday'].isna()))\n",
    "\n",
    "#Me fijo si faltan datos en TEST.\n",
    "# print('test[Customers]: NA = ', sum(testFile['Customers'].isna())) #No tiene Customers.\n",
    "print('test[Open]: NA = ', sum(testFile['Open'].isna())) #11\n",
    "print('test[Promo]: NA = ', sum(testFile['Promo'].isna()))\n",
    "print('test[StateHoliday]: NA = ', sum(testFile['StateHoliday'].isna()))\n",
    "print('test[SchoolHoliday]: NA = ', sum(testFile['SchoolHoliday'].isna()))\n",
    "\n",
    "#Me fijo si faltan datos en STORE.\n",
    "print('store[StoreType]: NA = ', sum(storeFile['StoreType'].isna()))\n",
    "print('store[Assortment]: NA = ', sum(storeFile['Assortment'].isna()))\n",
    "print('store[CompetitionDistance]: NA = ', sum(storeFile['CompetitionDistance'].isna())) #3\n",
    "print('store[Promo2]: NA = ', sum(storeFile['Promo2'].isna()))\n",
    "\n",
    "# print(trainFile['StateHoliday'].value_counts())\n",
    "# print(sum(trainFile['StateHoliday'] == 0))\n",
    "# print(sum(trainFile['StateHoliday'] == '0'))\n",
    "\n",
    "# trainFile['StateHoliday'] = trainFile['StateHoliday'].astype(str)\n",
    "\n",
    "# print(trainFile['StateHoliday'].value_counts())\n",
    "# print(sum(trainFile['StateHoliday'] == 0))\n",
    "# print(sum(trainFile['StateHoliday'] == '0'))\n",
    "\n",
    "# print(storeFile['CompetitionOpenSinceYear'].value_counts())\n",
    "# print(len(storeFile['CompetitionOpenSinceYear']))\n",
    "# print(storeFile['CompetitionDistance'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento de datos\n",
    "\n",
    "En cada data set levantado previamente, completamos con los datos faltantes teniendo en cuenta los siguientes criterios:\n",
    "- En el campo \"Open\": Este campo puede tomar los valores de 1 (Abierto) y 0 (Cerrado). En caso de que no se haya indicado nada en este campo, se lo considera como cerrado (0).\n",
    "- En el campo \"Competition Distance\": Dado que hay negocios de competencia que no existian previamente a la fecha de inicio de datos, solo hay informacion de distancia a la que se encuentra esa competencia a partir del momento en el que se establecio en su lugar geografico. Como necesitamos que haya informacion correspondiente a la distancia a la que se encuentra la competencia desde el inicio de fecha de los datos del data set, es necesario completar con informacion las fechas que no tienen dato. Si un local de competencia no existe aun, no tiene impacto en las ventas de nuestros locales y lo mismo ocurre con un local de competencia que geograficamente se encuentra muy lejos. Por eso se decidio tomar como criterio que para las fechas en las que la competencia aun no existia, la distancia sea completada con un valor muy grande de distancia (como si estuviera muy lejos). Este valor elegido es el maximo encontrado en el dataset.\n",
    "- Tanto en el campo \"CompetitionOpenSinceYear\" como en el campo \"CompetitionOpenSinceMonth\": En caso de que no haya informacion en estos campos, se asume que la competencia ya existia para la fecha de los primeros datos del dataset, por lo que se le asigna a cada campo el dato minimo presente en el dataset. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Completamos los datos en todos los archivos.\n",
    "\n",
    "#trainFile.loc[trainFile['Open'].isna(), 'Open'] = 0\n",
    "testFile.loc[testFile['Open'].isna(), 'Open'] = 0\n",
    "\n",
    "storeFile.loc[storeFile['CompetitionDistance'].isna(), 'CompetitionDistance'] = storeFile['CompetitionDistance'].max()\n",
    "storeFile.loc[storeFile['CompetitionOpenSinceYear'].isna(), 'CompetitionOpenSinceYear'] = storeFile['CompetitionOpenSinceYear'].min()\n",
    "storeFile.loc[storeFile['CompetitionOpenSinceMonth'].isna(), 'CompetitionOpenSinceMonth'] = storeFile['CompetitionOpenSinceMonth'].min()\n",
    "\n",
    "# print(trainFile['Date'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "En esta parte pasamos las fechas al formato de fechas necesario para los analisis siguientes, empleando una funcion de Pandas. Pasamos los datos numericos a int32 con la finalidad de poder interpretarlos luego como numeros. Y borramos la columna (o campo) de \"Costumers\" presente en los datos levantados de train, ya que en test no estaban estos datos y buscamos que haya consistencia entre los campos de datos que tenemos de train y de test.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-87-7c7ca1c695bc>:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  trainFile['CompetitionDistance'][i] = storeFile['CompetitionDistance'][trainFile['Store'][i] - 1]\n",
      "<ipython-input-87-7c7ca1c695bc>:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  trainFile['CompetitionDistance'][i] = storeFile['CompetitionDistance'].max()\n",
      "<ipython-input-87-7c7ca1c695bc>:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  testFile['CompetitionDistance'][i] = storeFile['CompetitionDistance'][testFile['Store'][i] - 1]\n",
      "<ipython-input-87-7c7ca1c695bc>:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  testFile['CompetitionDistance'][i] = storeFile['CompetitionDistance'].max()\n"
     ]
    }
   ],
   "source": [
    "#Empiezamos a preprocesar los datos.\n",
    "\n",
    "import datetime\n",
    "\n",
    "trainFile['Date'] = pd.to_datetime(trainFile.Date)\n",
    "testFile['Date'] = pd.to_datetime(testFile.Date)\n",
    "storeFile['CompetitionOpenSinceYear'] = storeFile['CompetitionOpenSinceYear'].astype(np.int32)\n",
    "storeFile['CompetitionOpenSinceMonth'] = storeFile['CompetitionOpenSinceMonth'].astype(np.int32)\n",
    "storeFile['CompetitionOpenSince'] = pd.to_datetime(storeFile.apply(lambda x: datetime.datetime(x.CompetitionOpenSinceYear, x.CompetitionOpenSinceMonth, 1), axis=1))\n",
    "\n",
    "trainFile['CompetitionDistance'] = np.zeros(len(trainFile['Date']))\n",
    "for i in range(len(trainFile['Date'])):\n",
    "    if(trainFile['Date'][i] > storeFile['CompetitionOpenSince'][trainFile['Store'][i] - 1]):\n",
    "        trainFile['CompetitionDistance'][i] = storeFile['CompetitionDistance'][trainFile['Store'][i] - 1]\n",
    "    else:\n",
    "        trainFile['CompetitionDistance'][i] = storeFile['CompetitionDistance'].max()\n",
    "#trainFile['StateHoliday'] = ['0' if trainFile['StateHoliday'] == 0 else trainFile['StateHoliday']]\n",
    "trainFile['StateHoliday'] = trainFile['StateHoliday'].astype(str)\n",
    "trainFile['SchoolHoliday'] = trainFile.SchoolHoliday != '0' #Convertimos strings a bool (ya que es binario).\n",
    "#trainFile['SchoolHoliday'] = trainFile['SchoolHoliday'].astype(np.int8)\n",
    "del trainFile['Customers'] #Borramos la columna costumers ya que el test no lo tiene (?).\n",
    "#print(trainFile.head())\n",
    "\n",
    "testFile['CompetitionDistance'] = np.zeros(len(testFile['Date']))\n",
    "for i in range(len(testFile['Date'])):\n",
    "    if(testFile['Date'][i] > storeFile['CompetitionOpenSince'][testFile['Store'][i] - 1]):\n",
    "        testFile['CompetitionDistance'][i] = storeFile['CompetitionDistance'][testFile['Store'][i] - 1]\n",
    "    else:\n",
    "        testFile['CompetitionDistance'][i] = storeFile['CompetitionDistance'].max()\n",
    "#trainFile['StateHoliday'] = ['0' if trainFile['StateHoliday'] == 0 else trainFile['StateHoliday']]\n",
    "testFile['StateHoliday'] = testFile['StateHoliday'].astype(str)\n",
    "testFile['SchoolHoliday'] = testFile.SchoolHoliday != '0' #Convertimos strings a bool (ya que es binario).\n",
    "#testFile['SchoolHoliday'] = testFile['SchoolHoliday'].astype(np.int8)\n",
    "#print(testFile.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Store  DayOfWeek       Date  Sales  Open  Promo StateHoliday  \\\n",
      "0      1          5 2015-07-31   5263     1      1            0   \n",
      "1      2          5 2015-07-31   6064     1      1            0   \n",
      "2      3          5 2015-07-31   8314     1      1            0   \n",
      "3      4          5 2015-07-31  13995     1      1            0   \n",
      "4      5          5 2015-07-31   4822     1      1            0   \n",
      "\n",
      "   SchoolHoliday  CompetitionDistance  \n",
      "0           True               1270.0  \n",
      "1           True                570.0  \n",
      "2           True              14130.0  \n",
      "3           True                620.0  \n",
      "4           True              29910.0  \n",
      "   Id  Store  DayOfWeek       Date  Open  Promo StateHoliday  SchoolHoliday  \\\n",
      "0   1      1          4 2015-09-17   1.0      1            0           True   \n",
      "1   2      3          4 2015-09-17   1.0      1            0           True   \n",
      "2   3      7          4 2015-09-17   1.0      1            0           True   \n",
      "3   4      8          4 2015-09-17   1.0      1            0           True   \n",
      "4   5      9          4 2015-09-17   1.0      1            0           True   \n",
      "\n",
      "   CompetitionDistance  \n",
      "0               1270.0  \n",
      "1              14130.0  \n",
      "2              24000.0  \n",
      "3               7520.0  \n",
      "4               2030.0  \n"
     ]
    }
   ],
   "source": [
    "print(trainFile.head())\n",
    "print(testFile.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuacion agregamos algunas columnas de datos que se obtuvieron del archivo Store, a los datos de train y test. Los campos agregados son los siguientes:      \n",
    "- StoreType\n",
    "- Assortment\n",
    "- CompetitionDistance\n",
    "- Promo2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Store  DayOfWeek       Date  Sales  Open  Promo StateHoliday  \\\n",
      "0      1          5 2015-07-31   5263     1      1            0   \n",
      "1      2          5 2015-07-31   6064     1      1            0   \n",
      "2      3          5 2015-07-31   8314     1      1            0   \n",
      "3      4          5 2015-07-31  13995     1      1            0   \n",
      "4      5          5 2015-07-31   4822     1      1            0   \n",
      "\n",
      "   SchoolHoliday  CompetitionDistance StoreType Assortment  Promo2  \n",
      "0           True               1270.0         c          a       0  \n",
      "1           True                570.0         a          a       1  \n",
      "2           True              14130.0         a          a       1  \n",
      "3           True                620.0         c          c       0  \n",
      "4           True              29910.0         a          a       0  \n",
      "   Store  DayOfWeek       Date  Open  Promo StateHoliday  SchoolHoliday  \\\n",
      "0      1          4 2015-09-17   1.0      1            0           True   \n",
      "1      3          4 2015-09-17   1.0      1            0           True   \n",
      "2      7          4 2015-09-17   1.0      1            0           True   \n",
      "3      8          4 2015-09-17   1.0      1            0           True   \n",
      "4      9          4 2015-09-17   1.0      1            0           True   \n",
      "\n",
      "   CompetitionDistance StoreType Assortment  Promo2  \n",
      "0               1270.0         c          a       0  \n",
      "1              14130.0         a          a       1  \n",
      "2              24000.0         a          c       0  \n",
      "3               7520.0         a          a       0  \n",
      "4               2030.0         a          c       0  \n"
     ]
    }
   ],
   "source": [
    "#Juntamos información del storeFile a trainFile y a testFile.\n",
    "\n",
    "train = trainFile.merge(storeFile, how='left', left_on='Store', right_on='Store', suffixes=(\"\", \"_y\"))\n",
    "del train['CompetitionDistance_y']\n",
    "del train['CompetitionOpenSinceMonth']\n",
    "del train['CompetitionOpenSinceYear']\n",
    "#del train['Promo2']\n",
    "del train['Promo2SinceWeek']\n",
    "del train['Promo2SinceYear']\n",
    "del train['PromoInterval']\n",
    "del train['CompetitionOpenSince']\n",
    "#print(len(train[train.StoreType.isnull()]))\n",
    "print(train.head())\n",
    "\n",
    "test = testFile.merge(storeFile, how='left', left_on='Store', right_on='Store', suffixes=(\"\", \"_y\"))\n",
    "del test['Id']\n",
    "del test['CompetitionDistance_y']\n",
    "del test['CompetitionOpenSinceMonth']\n",
    "del test['CompetitionOpenSinceYear']\n",
    "#del test['Promo2']\n",
    "del test['Promo2SinceWeek']\n",
    "del test['Promo2SinceYear']\n",
    "del test['PromoInterval']\n",
    "del test['CompetitionOpenSince']\n",
    "#print(len(test[train.StoreType.isnull()]))\n",
    "print(test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('trainPreproc.csv', index=False)\n",
    "test.to_csv('testPreproc.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tobias Scala\\anaconda3\\envs\\rn\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3441: DtypeWarning: Columns (6) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train = pd.read_csv('trainPreproc.csv')\n",
    "#print(train.columns)\n",
    "test = pd.read_csv('testPreproc.csv')\n",
    "#print(test.columns)\n",
    "\n",
    "train['StateHoliday'] = train['StateHoliday'].astype(str)\n",
    "test['StateHoliday'] = test['StateHoliday'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuacion se normalizan las variables continuas empleando la ecuacion:     \n",
    "$\\frac{x_i-\\mu}{\\sigma}$\n",
    "siendo $x_i$ cada uno de los datos, $\\mu$ la media y $\\sigma$ el desvio estandar.\n",
    "Tambien se aplica hot encoder a las variables categoricas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Store  DayOfWeek  StateHoliday  StoreType  Assortment\n",
      "0            0          4             0          2           0\n",
      "1            1          4             0          0           0\n",
      "2            2          4             0          0           0\n",
      "3            3          4             0          2           2\n",
      "4            4          4             0          0           0\n",
      "...        ...        ...           ...        ...         ...\n",
      "1017204   1110          1             1          0           0\n",
      "1017205   1111          1             1          2           2\n",
      "1017206   1112          1             1          0           2\n",
      "1017207   1113          1             1          0           2\n",
      "1017208   1114          1             1          3           2\n",
      "\n",
      "[1017209 rows x 5 columns]\n",
      "             Open  CompetitionDistance     Promo  SchoolHoliday    Promo2\n",
      "0        0.452399            -0.484440  1.273237            0.0 -1.001128\n",
      "1        0.452399            -0.517645  1.273237            0.0  0.998873\n",
      "2        0.452399             0.125582  1.273237            0.0  0.998873\n",
      "3        0.452399            -0.515274  1.273237            0.0 -1.001128\n",
      "4        0.452399             0.874116  1.273237            0.0 -1.001128\n",
      "...           ...                  ...       ...            ...       ...\n",
      "1017204 -2.210440             3.053783 -0.785400            0.0  0.998873\n",
      "1017205 -2.210440            -0.455505 -0.785400            0.0 -1.001128\n",
      "1017206 -2.210440            -0.105430 -0.785400            0.0 -1.001128\n",
      "1017207 -2.210440            -0.503415 -0.785400            0.0 -1.001128\n",
      "1017208 -2.210440            -0.290903 -0.785400            0.0  0.998873\n",
      "\n",
      "[1017209 rows x 5 columns]\n",
      "       Store  DayOfWeek  StateHoliday  StoreType  Assortment\n",
      "0          0          3             0          2           0\n",
      "1          2          3             0          0           0\n",
      "2          6          3             0          0           2\n",
      "3          7          3             0          0           0\n",
      "4          8          3             0          0           2\n",
      "...      ...        ...           ...        ...         ...\n",
      "41083   1110          5             0          0           0\n",
      "41084   1111          5             0          2           2\n",
      "41085   1112          5             0          0           2\n",
      "41086   1113          5             0          0           2\n",
      "41087   1114          5             0          3           2\n",
      "\n",
      "[41088 rows x 5 columns]\n",
      "           Open  CompetitionDistance     Promo  SchoolHoliday    Promo2\n",
      "0      0.452399            -0.484440  1.273237            0.0 -1.001128\n",
      "1      0.452399             0.125582  1.273237            0.0  0.998873\n",
      "2      0.452399             0.593771  1.273237            0.0 -1.001128\n",
      "3      0.452399            -0.187968  1.273237            0.0 -1.001128\n",
      "4      0.452399            -0.448389  1.273237            0.0 -1.001128\n",
      "...         ...                  ...       ...            ...       ...\n",
      "41083  0.452399            -0.454556 -0.785400            0.0  0.998873\n",
      "41084  0.452399            -0.455505 -0.785400            0.0 -1.001128\n",
      "41085  0.452399            -0.105430 -0.785400            0.0 -1.001128\n",
      "41086  0.452399            -0.503415 -0.785400            0.0 -1.001128\n",
      "41087  0.452399            -0.290903 -0.785400            0.0  0.998873\n",
      "\n",
      "[41088 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "#Normalizamos las variables continuas y hacemos hot encoder a las variables categóricas.\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from keras.utils import np_utils\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "\n",
    "catColumns = ['Store', 'DayOfWeek', 'StateHoliday', 'StoreType', 'Assortment']\n",
    "contColumns = ['Open', 'CompetitionDistance', 'Promo', 'SchoolHoliday', 'Promo2']\n",
    "\n",
    "# for i in cont:\n",
    "#     xmean = train[i].mean()\n",
    "#     xstd = train[i].std()\n",
    "#     train[i] = (train[i].values - xmean)/xstd\n",
    "\n",
    "# enc = LabelEncoder()\n",
    "# for i in cat:\n",
    "#     enc.fit(train[i])\n",
    "#     train_ = enc.transform(train[i])\n",
    "#     train_ = np_utils.to_categorical(train_)\n",
    "#     print(train_)\n",
    "\n",
    "# print(train.head(train))\n",
    "\n",
    "catEnc = [(i, LabelEncoder()) for i in catColumns]\n",
    "contScal = [([i], StandardScaler()) for i in contColumns] #Se estandarizan las variables continuas.\n",
    "\n",
    "catMapp = DataFrameMapper(catEnc) #Mapea la columna que tienen las variables categóricas (string) en un número de índice (números). [a, b, a, d] = [0, 1, 0, 2]\n",
    "catMapp.fit(train) #Se arma un one hot vector. El fit asigna la cantidad de clases (?)\n",
    "contMapp = DataFrameMapper(contScal)\n",
    "contMapp.fit(train) #Calcula la mdeia y el desvío del dataframe. x' = (x-u)/sigma.\n",
    "\n",
    "train[catColumns] = catMapp.transform(train)\n",
    "test[catColumns] = catMapp.transform(test)\n",
    "train[contColumns] = contMapp.transform(train)\n",
    "test[contColumns] = contMapp.transform(test)\n",
    "print(train[catColumns])\n",
    "print(train[contColumns])\n",
    "print(test[catColumns])\n",
    "print(test[contColumns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Embeddings.\n",
    "\n",
    "# from tensorflow.keras.models import Model\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# from tensorflow.keras.layers import Embedding, Input, Flatten, Dense, Concatenate\n",
    "\n",
    "# catDict = {'Store': 50, 'DayOfWeek': 4, 'StateHoliday': 2, 'StoreType': 2, 'Assortment': 3}\n",
    "\n",
    "# storeIn = Input(shape=(1,), name=\"storeIn\")\n",
    "# storeEmb = Embedding(1115, catDict['Store'], name=\"storeEmb\")(storeIn)\n",
    "# storeFlat = Flatten(name=\"storeFlat\")(storeEmb)\n",
    "# dayofweekIn = Input(shape=(1,), name=\"dayofweekIn\")\n",
    "# dayofweekEmb = Embedding(7, catDict['DayOfWeek'], name=\"dayofweekEmb\")(dayofweekIn)\n",
    "# dayofweekFlat = Flatten(name=\"dayofweekFlat\")(dayofweekEmb)\n",
    "# stateholidayIn = Input(shape=(1,), name=\"stateholidayIn\")\n",
    "# stateholidayEmb = Embedding(4, catDict['StateHoliday'], name=\"stateholidayEmb\")(stateholidayIn)\n",
    "# stateholidayFlat = Flatten(name=\"stateholidayFlat\")(stateholidayEmb)\n",
    "# storetypeIn = Input(shape=(1,), name=\"storetypeIn\")\n",
    "# storetypeEmb = Embedding(4, catDict['StoreType'], name=\"storetypeEmb\")(storetypeIn)\n",
    "# storetypeFlat = Flatten(name=\"storetypeFlat\")(storetypeEmb)\n",
    "# assortmentIn = Input(shape=(1,), name=\"assortmentIn\")\n",
    "# assortmentEmb = Embedding(4, catDict['Assortment'], name=\"assortmentEmb\")(assortmentIn)\n",
    "# assortmentFlat = Flatten(name=\"assortmentFlat\")(assortmentEmb)\n",
    "\n",
    "# openIn = Input(shape=(1,), name=\"openIn\")\n",
    "# openDen = Dense(1, name=\"openDen\", activation = 'linear')(openIn)\n",
    "# competitiondistanceIn = Input(shape=(1,), name=\"competitiondistanceIn\")\n",
    "# competitiondistanceDen = Dense(1, name=\"competitiondistanceDen\", activation = 'linear')(competitiondistanceIn)\n",
    "# promoIn = Input(shape=(1,), name=\"promoIn\")\n",
    "# promoDen = Dense(1, name=\"promoDen\", activation = 'linear')(promoIn)\n",
    "# schoolholidayIn = Input(shape=(1,), name=\"schoolholidayIn\")\n",
    "# schoolholidayDen = Dense(1, name=\"schoolholidayDen\", activation = 'linear')(schoolholidayIn)\n",
    "# promo2In = Input(shape=(1,), name=\"promo2In\")\n",
    "# promo2Den = Dense(1, name=\"promo2Den\", activation = 'linear')(promo2In)\n",
    "\n",
    "# print(train['Store'].value_counts())\n",
    "# print(len(train['Store'].value_counts()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuacion, mediante embedding reducimos la cantidad de clases de una variable categorica.\n",
    "\n",
    "Dado que tenemos 1115 stores como fuente de datos, lo cual es una enorme cantidad, reducimos este numero y nos quedamos con 50. En cambio, para el resto de los campos de datos nos quedamos con la mitad de la cantidad de datos que tenemos originalmente. Este criterio, conocido como \"la regla del pulgar\", fue obtenida del video del siguiente link:    \n",
    "\n",
    "https://course18.fast.ai/lessonsml1/lesson12.html\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def catEmbeddings(catColumns, catDict):\n",
    "    catOut = []\n",
    "    catIn = []\n",
    "    for i in catColumns:\n",
    "        catIn_ = Input(shape=(1,))\n",
    "        catIn.append(catIn_)\n",
    "        catEmb = Embedding(len(train[i].value_counts()), catDict[i])(catIn_)\n",
    "        catFlat = Flatten()(catEmb)\n",
    "        catOut.append(catFlat)\n",
    "    return catIn, catOut    \n",
    "\n",
    "def contInput(contColumns):\n",
    "    contOut = []\n",
    "    for i in contColumns:\n",
    "        contIn = Input(shape=(1,))\n",
    "        contOut.append(contIn)\n",
    "    return contOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Embeddings.\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Embedding, Input, Flatten, Dense, Concatenate, Activation, Dropout\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "catDict = {'Store': 50, 'DayOfWeek': 4, 'StateHoliday': 2, 'StoreType': 2, 'Assortment': 3}\n",
    "\n",
    "catIn, catOut = catEmbeddings(catColumns, catDict)\n",
    "contIn = contInput(contColumns)\n",
    "\n",
    "merged = Concatenate()(catOut + contIn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo de procesamiento\n",
    "Realizamos el procesamiento empleando tanto la regularizacion L2 como Dropout y comparamos los resultados obtenidos a partir de cada uno de los casos.\n",
    "\n",
    "L2 y Dropout son metodos que se aplican para reducir la complejidad del modelo y disminuir el sobreajuste (overfitting), permitiendo una mayor generalizacion.\n",
    "\n",
    "- Regularizacion L2 (tambien conocida como la regularizacion de Ridge): Se agrega regularizacion agregandole un termino a la funcion de costo, la cual se busca minimizar. Este termino contiene una sumatoria de los pesos elevados al cuadrado, multiplicada por un hiperparametro. Si los pesos de ese termino que se agrega son grandes (los w), se agranda la funcion de costo y hay que penalizar esos pesos. L2 toma el cuadrado de cada peso y los suma. Este termino tambien tiene un hiperparametro (lambda) que es el mismo para todos los pesos y que determina la importancia que se le da a los pesos. Si lambda es cero, no hay regularizacion. Si es grande, predomina mas ese termino (y tampoco es la idea). Esta regularizacion se aplica a nivel de capa.\n",
    "\n",
    "- Dropout: Es otra tecnica de regularizacion para reducir el overfitting en redes neuronales. Se desprecian neuronas en forma aleatoria durante el entrenamiento de la red. Para lograr esto, generalmente se multiplica por cero algunas de las activaciones a la salida de una capa.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def neuralNetwork(merged):\n",
    "#     lambdaL2 = 1e-3 #Lambda para la regularización L2.\n",
    "#     model = Dense(1000, kernel_initializer=\"uniform\", kernel_regularizer=l2(lambdaL2))(merged)\n",
    "#     model = Activation('relu')(model) #'relu' so the cost function has less platous.\n",
    "#     model = Dense(1, activation='linear')(model) #'linear' so the output is not categorical.\n",
    "#     model = Model(catIn + contIn, [model])\n",
    "#     model.compile(optimizer='adam', metrics=['mse'], loss='mse')\n",
    "#     #model.compile(optimizer=Adam(lr=0.001), metrics=['mse', rmspe], loss='mse')\n",
    "#     return model\n",
    "\n",
    "def neuralNetwork(merged):\n",
    "    lambdaL2 = 1e-3 #Lambda para la regularización L2.\n",
    "    model = Dense(1000, kernel_initializer=\"uniform\", kernel_regularizer=l2(lambdaL2))(merged)\n",
    "    model = Activation('relu')(model) #'relu' so the cost function has less platous.\n",
    "    model = Dropout(0.2)(model)\n",
    "    model = Dense(1, activation='linear')(model) #'linear' so the output is not categorical.\n",
    "    model = Model(catIn + contIn, [model])\n",
    "    model.compile(optimizer='adam', metrics=['mse'], loss='mse')\n",
    "    #model.compile(optimizer=Adam(lr=0.001), metrics=['mse', rmspe], loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def neuralNetwork(merged):\n",
    "#     lambdaL2 = 1e-3 #Lambda para la regularización L2.\n",
    "#     model = Dense(1000, kernel_initializer=\"uniform\", kernel_regularizer=l2(lambdaL2), activation='relu')(merged) #'relu' so the cost function has less platous.\n",
    "#     model = Dense(1, activation='linear')(model) #'linear' so the output is not categorical.\n",
    "#     return Model(catIn + contIn, [model])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validacion\n",
    "Para esta etapa empleamos la metrica RMSPE, para cada uno de los modelos empleados previamente para el procesamiento ya que asi podremos compararlos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9594429463364953\n",
      "0.04055705366350475\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "train['Date'] = pd.to_datetime(train.Date)\n",
    "#test['Date'] = pd.to_datetime(test.Date)\n",
    "valid = train[train.Date >= datetime.datetime(2015, 6, 25)]\n",
    "train = train[train.Date < datetime.datetime(2015, 6, 25)]\n",
    "print(len(train)/(len(train)+len(valid)))\n",
    "print(len(valid)/(len(train)+len(valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmspeMetric(valid, predY):\n",
    "    rmspe = 0\n",
    "    for i in range(len(predY)):\n",
    "        if(valid['Sales'][i] != 0):\n",
    "            rmspe = rmspe + ((valid['Sales'][i] - predY[i])/valid['Sales'][i])**2\n",
    "    rmspe = np.sqrt(rmspe/len(predY))\n",
    "    return rmspe\n",
    "# def rmspeMetric(validY, predY):\n",
    "#     return np.sqrt((((validY - predY)/validY)**2).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparamos los datos de interés para entrenar a modelo. Dichos datos de interés son:\n",
    "- Store\n",
    "- DayOfWeek\n",
    "- StateHoliday\n",
    "- StoreType\n",
    "- Assortment\n",
    "- Open\n",
    "- CompetitionDistance\n",
    "- Promo\n",
    "- SchoolHoliday\n",
    "- Promo2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0.000e+00],\n",
      "       [1.000e+00],\n",
      "       [2.000e+00],\n",
      "       ...,\n",
      "       [1.112e+03],\n",
      "       [1.113e+03],\n",
      "       [1.114e+03]]), array([[2.],\n",
      "       [2.],\n",
      "       [2.],\n",
      "       ...,\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.]]), array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       ...,\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.]]), array([[2.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       ...,\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [3.]]), array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       ...,\n",
      "       [2.],\n",
      "       [2.],\n",
      "       [2.]]), array([[ 0.45239852],\n",
      "       [ 0.45239852],\n",
      "       [ 0.45239852],\n",
      "       ...,\n",
      "       [-2.21044047],\n",
      "       [-2.21044047],\n",
      "       [-2.21044047]]), array([[-0.4844404 ],\n",
      "       [-0.51764533],\n",
      "       [ 0.12558175],\n",
      "       ...,\n",
      "       [-0.10542975],\n",
      "       [-0.50341465],\n",
      "       [-0.29090304]]), array([[-0.78539979],\n",
      "       [-0.78539979],\n",
      "       [-0.78539979],\n",
      "       ...,\n",
      "       [-0.78539979],\n",
      "       [-0.78539979],\n",
      "       [-0.78539979]]), array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       ...,\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]]), array([[-1.00112823],\n",
      "       [ 0.99887304],\n",
      "       [ 0.99887304],\n",
      "       ...,\n",
      "       [-1.00112823],\n",
      "       [-1.00112823],\n",
      "       [ 0.99887304]])]\n",
      "[array([[0.000e+00],\n",
      "       [1.000e+00],\n",
      "       [2.000e+00],\n",
      "       ...,\n",
      "       [1.112e+03],\n",
      "       [1.113e+03],\n",
      "       [1.114e+03]]), array([[4.],\n",
      "       [4.],\n",
      "       [4.],\n",
      "       ...,\n",
      "       [3.],\n",
      "       [3.],\n",
      "       [3.]]), array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       ...,\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]]), array([[2.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       ...,\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [3.]]), array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       ...,\n",
      "       [2.],\n",
      "       [2.],\n",
      "       [2.]]), array([[0.45239852],\n",
      "       [0.45239852],\n",
      "       [0.45239852],\n",
      "       ...,\n",
      "       [0.45239852],\n",
      "       [0.45239852],\n",
      "       [0.45239852]]), array([[-0.4844404 ],\n",
      "       [-0.51764533],\n",
      "       [ 0.12558175],\n",
      "       ...,\n",
      "       [-0.10542975],\n",
      "       [-0.50341465],\n",
      "       [-0.29090304]]), array([[ 1.27323691],\n",
      "       [ 1.27323691],\n",
      "       [ 1.27323691],\n",
      "       ...,\n",
      "       [-0.78539979],\n",
      "       [-0.78539979],\n",
      "       [-0.78539979]]), array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       ...,\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]]), array([[-1.00112823],\n",
      "       [ 0.99887304],\n",
      "       [ 0.99887304],\n",
      "       ...,\n",
      "       [-1.00112823],\n",
      "       [-1.00112823],\n",
      "       [ 0.99887304]])]\n",
      "[array([[   0.],\n",
      "       [   2.],\n",
      "       [   6.],\n",
      "       ...,\n",
      "       [1112.],\n",
      "       [1113.],\n",
      "       [1114.]]), array([[3.],\n",
      "       [3.],\n",
      "       [3.],\n",
      "       ...,\n",
      "       [5.],\n",
      "       [5.],\n",
      "       [5.]]), array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       ...,\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]]), array([[2.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       ...,\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [3.]]), array([[0.],\n",
      "       [0.],\n",
      "       [2.],\n",
      "       ...,\n",
      "       [2.],\n",
      "       [2.],\n",
      "       [2.]]), array([[0.45239852],\n",
      "       [0.45239852],\n",
      "       [0.45239852],\n",
      "       ...,\n",
      "       [0.45239852],\n",
      "       [0.45239852],\n",
      "       [0.45239852]]), array([[-0.4844404 ],\n",
      "       [ 0.12558175],\n",
      "       [ 0.59377138],\n",
      "       ...,\n",
      "       [-0.10542975],\n",
      "       [-0.50341465],\n",
      "       [-0.29090304]]), array([[ 1.27323691],\n",
      "       [ 1.27323691],\n",
      "       [ 1.27323691],\n",
      "       ...,\n",
      "       [-0.78539979],\n",
      "       [-0.78539979],\n",
      "       [-0.78539979]]), array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       ...,\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]]), array([[-1.00112823],\n",
      "       [ 0.99887304],\n",
      "       [-1.00112823],\n",
      "       ...,\n",
      "       [-1.00112823],\n",
      "       [-1.00112823],\n",
      "       [ 0.99887304]])]\n"
     ]
    }
   ],
   "source": [
    "columns = catColumns + contColumns\n",
    "trainX = np.hsplit(train[columns].values, len(columns))\n",
    "validX = np.hsplit(valid[columns].values, len(columns))\n",
    "testX = np.hsplit(test[columns].values, len(columns))\n",
    "\n",
    "print(trainX)\n",
    "print(validX)\n",
    "print(testX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizamos los datos correspondientes a la salida de nuestro modelo (Sales) tanto en el datframe del train como de validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.01835703  0.0210813  -0.00853301 ... -0.16756264 -0.52005182\n",
      " -0.15797597]\n",
      "[ 0.05341483  0.06451466  0.08626561 ...  0.00542481 -0.05094384\n",
      " -0.04227365]\n"
     ]
    }
   ],
   "source": [
    "#print(train.groupby('Store').mean()['Sales'])\n",
    "trainY = (train['Sales'].values - train.groupby('Store').mean()['Sales'][train['Store']].values)\n",
    "trainYmax = trainY.max()\n",
    "trainY = trainY/trainYmax\n",
    "#trainY = np.hsplit(trainY, trainY.shape[1])\n",
    "print(trainY)\n",
    "validY = (valid['Sales'].values - valid.groupby('Store').mean()['Sales'][valid['Store']].values)\n",
    "validYmax = validY.max()\n",
    "validY = validY/validYmax\n",
    "#validY = np.hsplit(validY, validY.shape[1])\n",
    "print(validY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento\n",
    "Se entrena el modelo propuesto con los métodos de regularización mencionados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "3813/3813 - 26s - loss: 0.0022 - mse: 0.0016\n",
      "Epoch 2/20\n",
      "3813/3813 - 27s - loss: 0.0016 - mse: 0.0014\n",
      "Epoch 3/20\n",
      "3813/3813 - 27s - loss: 0.0016 - mse: 0.0014\n",
      "Epoch 4/20\n",
      "3813/3813 - 28s - loss: 0.0016 - mse: 0.0014\n",
      "Epoch 5/20\n",
      "3813/3813 - 27s - loss: 0.0015 - mse: 0.0014\n",
      "Epoch 6/20\n",
      "3813/3813 - 29s - loss: 0.0015 - mse: 0.0014\n",
      "Epoch 7/20\n",
      "3813/3813 - 27s - loss: 0.0015 - mse: 0.0014\n",
      "Epoch 8/20\n",
      "3813/3813 - 27s - loss: 0.0015 - mse: 0.0014\n",
      "Epoch 9/20\n",
      "3813/3813 - 27s - loss: 0.0015 - mse: 0.0014\n",
      "Epoch 10/20\n",
      "3813/3813 - 27s - loss: 0.0015 - mse: 0.0014\n",
      "Epoch 11/20\n",
      "3813/3813 - 27s - loss: 0.0015 - mse: 0.0014\n",
      "Epoch 12/20\n",
      "3813/3813 - 26s - loss: 0.0015 - mse: 0.0014\n",
      "Epoch 13/20\n",
      "3813/3813 - 26s - loss: 0.0015 - mse: 0.0014\n",
      "Epoch 14/20\n",
      "3813/3813 - 27s - loss: 0.0015 - mse: 0.0014\n",
      "Epoch 15/20\n",
      "3813/3813 - 24s - loss: 0.0015 - mse: 0.0014\n",
      "Epoch 16/20\n",
      "3813/3813 - 23s - loss: 0.0015 - mse: 0.0014\n",
      "Epoch 17/20\n",
      "3813/3813 - 24s - loss: 0.0015 - mse: 0.0014\n",
      "Epoch 18/20\n",
      "3813/3813 - 32s - loss: 0.0015 - mse: 0.0014\n",
      "Epoch 19/20\n",
      "3813/3813 - 31s - loss: 0.0015 - mse: 0.0014\n",
      "Epoch 20/20\n",
      "3813/3813 - 32s - loss: 0.0015 - mse: 0.0014\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x21b8223c580>"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = neuralNetwork(merged)\n",
    "#model.fit(trainX, trainY, validation_data=(validX, validY),  epochs=20, batch_size=256, verbose=2)\n",
    "model.fit(trainX, trainY, epochs=20, batch_size=256, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluamos nuestro modelo con validación ya que se conocen los valores reales de salida (Sales). Este paso es importante ya que, dependiendo que tan bien funciona nuestro modelo, se proseguirá con la predicción de la salida (Sales) con el data frame de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30499/30499 [==============================] - 68s 2ms/step - loss: 0.0014 - mse: 0.0013\n",
      "0.0013035677839070559\n",
      "1290/1290 [==============================] - 4s 3ms/step - loss: 0.0022 - mse: 0.0021\n",
      "0.002109960885718465\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(trainX, trainY)\n",
    "print(acc)\n",
    "\n",
    "loss, acc = model.evaluate(validX, validY)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se predice la salida (Sales) del data frame de test y luego se lo guarda en un archivo csv para hacer submit en Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1290/1290 [==============================] - 2s 2ms/step\n",
      "0.8174407271455243\n",
      "1284/1284 [==============================] - 6s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "predY = model.predict(validX, verbose = 1)[:,0]*trainYmax\n",
    "rmspe = rmspeMetric(valid, predY)\n",
    "print(rmspe)\n",
    "\n",
    "predY = model.predict(testX, verbose = 1)[:,0]*trainYmax\n",
    "# rmspe = rmspeMetric(test, predY)\n",
    "# print(rmspe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('sample_submission.csv')\n",
    "submission['Sales'] = predY\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "352c8700b9e48b360dd8f8c948f408188979ddb3d67f3cf144936d3d11d35124"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('rn': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
